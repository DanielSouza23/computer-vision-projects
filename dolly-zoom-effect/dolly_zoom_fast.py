import numpy as np
import pickle
import time
from pathlib import Path 
import matplotlib.pyplot as plt


import cv2
import open3d as o3d

# I actually didn't know how to use open3d, and since it wasn't really a part of the assignment's solution/course material, I asked chatGPT for help on this part:
# The code below is mostly generated by chatGPT (setup_renderer and PointCloud2Image functions).

'''
Error I previously had when I ran the original code:
 File "c:\Users\rsouz\Downloads\dolly_zoom_fast-1 (1).py", line 71, in SampleCameraPath
    renderer = setup_renderer(points, height, width)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\rsouz\Downloads\dolly_zoom_fast-1 (1).py", line 20, in setup_renderer
    renderer = o3d.visualization.rendering.OffscreenRenderer(width, height)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
RuntimeError: [Open3D Error] (__cdecl open3d::visualization::rendering::EngineInstance::EngineInstance(void)) 
D:\a\Open3D\Open3D\cpp\open3d\visualization\rendering\filament\FilamentEngine.cpp:104: EGL Headless is not supported on this platform.
'''
def setup_renderer(points, height, width):
    pts = o3d.utility.Vector3dVector(points[:3,:].transpose())
    colors = o3d.utility.Vector3dVector(points[3:,:].transpose())
    cloud = o3d.geometry.PointCloud()
    cloud.points = pts
    cloud.colors = colors

    vis = o3d.visualization.Visualizer()
    vis.create_window(visible=False, width=width, height=height)
    vis.add_geometry(cloud)
    opt = vis.get_render_option()
    opt.background_color = np.asarray([0,0,0])
    opt.point_size = 1.0
    opt.light_on = False
    vis.poll_events()
    vis.update_renderer()

    return vis


def PointCloud2Image(K, P, renderer, height, width, w):
    intrins = o3d.camera.PinholeCameraIntrinsic(width, height, K[0,0], K[1,1], K[0,2], K[1,2])
    P = np.vstack((P,np.asarray([[0,0,0,1]])))

    params = o3d.camera.PinholeCameraParameters()
    params.intrinsic = intrins
    params.extrinsic = P
    ctr = renderer.get_view_control()
    ctr.convert_from_pinhole_camera_parameters(params, allow_arbitrary=True)

    renderer.poll_events()
    renderer.update_renderer()

    image = np.asarray(renderer.capture_screen_float_buffer(do_render=False))
    image = (image * 255.0).clip(0,255).astype(np.uint8)
    image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR) 

    return image

# Step 1
def scale_intrinsics(K, scale):
    K = K.copy()
    K[0,0] *= scale
    K[1,1] *= scale
    K[0,2] *= scale
    K[1,2] *= scale
    return K


def SampleCameraPath():
    # load object file to retrieve data
    file_p = open("data/data.obj",'rb')
    camera_objs = pickle.load(file_p)

    # extract objects from object array
    (top, left, height, width) = camera_objs[0].flatten()
    (w,_) = camera_objs[1].flatten()
    K = camera_objs[2]
    R = np.identity(3)
    scale = 0.5 
    zoom_iters = 40
    zoom = np.array([0,0,0.0]).reshape((3,1))

    K = scale_intrinsics(K, scale) # Scaling intrinsics parameters accordingly before scaling image
    f = K[0,0]
    height, width = int(height*scale), int(width*scale) # 0.5 of W * 0.5 of H = 0.25 of original image size

    # setup point cloud
    ForegroundPointCloudRGB = camera_objs[3]
    BackgroundPointCloudRGB = camera_objs[4]
    points = np.hstack((BackgroundPointCloudRGB,ForegroundPointCloudRGB))

    # get average foreground Z distance (should be ~4m + zoom)
    Z_avg = np.mean(ForegroundPointCloudRGB[2,:]) + zoom[2,0]
    
    # Step 2.1: scene depth
    Z = ForegroundPointCloudRGB[2, :].astype(float)
    Z_ref = np.median(Z)
    Z_min = np.percentile(Z, 5)
    Z_max = np.percentile(Z, 95)
    # Step 2.2: calculate baseline
    d = 32
    B = (d * Z_ref)/f
    # Step 2.3: min and max d
    dmin = f * B / Z_max
    dmax = f * B / Z_min
    # bound
    dmin_ = int(np.floor(dmin)) 
    if dmin_ < 0: # no negatives, min = 0 worst case 
        dmin_ = 0
    dmax_ = int(np.ceil(dmax))
    print("Disparity range: [%d, %d]\n", dmin_, dmax_)

    K_stereo = K.copy()   
    
    # setup renderer for point cloud
    renderer = setup_renderer(points, height, width)

    # loops from 0.25 to 1.77 in 0.02 increments (75 images)
    img_array = []
    count = 0
    steps = np.linspace(0.25,1.77,num=zoom_iters)
    for step in steps:
        tic = time.time()

        fname = "SampleOutput{}.jpg".format(count)
        print("\nGenerating {}".format(fname))
        count += 1

        # adjust z steps
        t = np.array([0,0,step]).reshape((3,1))

        # get change from original distance Z
        dZ = step

        # apply constant zoom factor to image
        t += zoom

        # calculate and set new focal length
        fp = f*((Z_avg+dZ)/Z_avg)
        K[0,0] = fp
        K[1,1] = fp

        # create extrinsics matrix
        P = np.hstack((R,t))

        # create new image after projection
        img = PointCloud2Image(K, P, renderer, height, width, w)

        # write image to file 'fname'
        cv2.imwrite(fname,img)

        # append image to img_array
        img_array.append(img.astype(np.uint8))

        toc = time.time()
        toc = toc-tic
        print("{0:.4g} s".format(toc))

    # get image specs
    height,width,_ = img_array[0].shape
    size = (width,height)

    # create video writer object
    vid = cv2.VideoWriter("dolly_zoom.mp4", cv2.VideoWriter_fourcc(*'mp4v'),15,size)

    # write each frame to the video
    for i in range(len(img_array)):
        vid.write(img_array[i])
    vid.release()

    R_stereo = np.eye(3)
    t_left  = np.array([0.0, 0.0, 0.0]).reshape(3,1)
    t_right = np.array([B, 0.0, 0.0]).reshape(3,1)   
    P_left  = np.hstack((R_stereo, t_left))
    P_right = np.hstack((R_stereo, t_right))

    img_left  = PointCloud2Image(K_stereo, P_left,  renderer, height, width, w)
    img_right = PointCloud2Image(K_stereo, P_right, renderer, height, width, w)

    cv2.imwrite("left.png",  img_left)
    cv2.imwrite("right.png", img_right)

    gray_l  = (0.30*img_left[:,:,2] + 0.59*img_left[:,:,1] + 0.11*img_left[:,:,0]).astype(np.float32)
    gray_r = (0.30*img_right[:,:,2] + 0.59*img_right[:,:,1] + 0.11*img_right[:,:,0]).astype(np.float32)

    gray_left = np.clip(gray_l,  0, 255).astype(np.uint8)
    gray_right = np.clip(gray_r, 0, 255).astype(np.uint8)

    cv2.imwrite("left_gray.png",  gray_left)
    cv2.imwrite("right_gray.png", gray_right)

    # Copy and Paste from P1
    def rank5(img):
        r = 2 # 5x5
        H, W = img.shape
        out = np.zeros((H, W), np.uint8) # Border    
        for y in range(r, H-r): # only where a 5×5 - this means that image will have a black border of 2 pixels (not applicable for 5x5 as it wont have enough neighbors)
            for x in range(r, W-r):
                center = img[y, x]
                win = img[y-r:y+r+1, x-r:x+r+1] # 5×5 window around pixel
                out[y, x] = np.sum(win < center)   # count how many neighbors are strictly less than center
        return out

    def census5(img):
        r = 2 # 5x5
        H, W = img.shape
        out = np.zeros((H, W), np.uint32)
        for y in range(r, H - r): # only where 5x5 fits
            for x in range(r, W - r):           
                code = 0
                for dy in range(-r, r + 1):    # top-left to bottom-right
                    for dx in range(-r, r + 1):
                        if dy == 0 and dx == 0:
                            continue # skip center
                        code <<= 1 # make room for next bit (new LSB, shift order to the left)
                        if img[y + dy, x + dx] < img[y, x]:
                            code |= 1 # x-20, example (slide 55)
                        else:
                            code |= 0 # x+35, example 
                out[y, x] = code # output bit string, 1s and 0s, top left to bottom right            
        return out

    def hamming(L, R):
        return int((int(L) ^ int(R)).bit_count()) # Compare L and R, see where bit strings differ, and count how many differences

    def cost(L_cen, R_cen, d): # Cost  for each disparity
        H, W = L_cen.shape
        cost = np.full((H, W), 24, np.uint8)  # initialize with max possible Hamming cost (24) for unaccounted pixels
        x_start = d  # d = xl - xr
        for y in range(H): # row search
            for x in range(x_start, W): # compare pixels along epipolar line (shifted by d) 
                cost[y, x] = hamming(L_cen[y, x], R_cen[y, x - d])
        return cost

    def sums(img, win): # Aggregation (Slide 32)
        r = win // 2 # windows = 5, 9, 15
        H, W = img.shape
        out = np.zeros_like(img, dtype=np.int32) # array of 0s same size as img
        for y in range(H): 
            for x in range(W):
                # Create box around pixel
                t_ = (y-r) # tentative
                if t_ < 0:
                    top = 0
                else:
                    top = t_
                b_ = (y+r+1) # tentative
                if b_ > H:
                    bottom = H
                else:
                    bottom = b_
                r_ = (x+r+1) # tentative
                if r_ > W:
                    right = W
                else:
                    right = r_
                l_ = (x-r) # tentative
                if l_ < 0:
                    left = 0
                else:
                    left = l_
                out[y, x] = img[top:bottom, left:right].sum() # Sum costs within box
        return out

    def WTA(L_cen, R_cen, min_disp, max_disp, win):
        H, W = L_cen.shape
        best = np.full((H, W), np.iinfo(np.int32).max, dtype=np.int32)   # store smallest aggregated cost, initial high value to easily be replaced
        disp = np.zeros((H, W), np.uint8) # result disparity for single left image pixel

        for d in range(min_disp, max_disp + 1):
            cost_d = cost(L_cen, R_cen, d) # cost
            agg_d  = sums(cost_d, win) # aggregated cost 
            
            for y in range(H):  
                for x in range(W):
                    # Compare calculated cost with current best
                    if agg_d[y,x] < best[y,x]:
                        best[y,x] = agg_d[y,x]
                        disp[y,x] = d
        return disp

    def stereo(left_gray,right_gray, min_disp, max_disp, win):
        # Rank and Census
        L_rank = rank5(left_gray)
        R_rank = rank5(right_gray)
        L_census = census5(L_rank)
        R_cen = census5(R_rank)
        # WTA 
        disp = WTA(L_census, R_cen, min_disp, max_disp, win)
        return disp

    win = 5
    disp5_ = stereo(gray_left, gray_right, dmin_, dmax_, win)

    disp5 = ((disp5_.astype(np.float32) - dmin_) / (dmax_ - dmin_)) * 255.0  # Scale to 0-255 range
    disp5 = np.clip(disp5, 0, 255).astype(np.uint8)
    
    win = 9
    disp9_ = stereo(gray_left, gray_right, dmin_, dmax_, win)

    disp9 = ((disp9_.astype(np.float32) - dmin_) / (dmax_ - dmin_)) * 255.0 
    disp9 = np.clip(disp9, 0, 255).astype(np.uint8)

    win = 15
    disp15_ = stereo(gray_left, gray_right, dmin_, dmax_, win)

    disp15 = ((disp15_.astype(np.float32) - dmin_) / (dmax_ - dmin_)) * 255.0  # Scale to 0-255 range
    disp15 = np.clip(disp15, 0, 255).astype(np.uint8)

    fig, ax = plt.subplots(1, 3, figsize=(12, 4))
    for a, img, title in zip(ax, (disp5, disp9, disp15), ("5×5", "9×9", "15×15")):
        a.imshow(img, cmap="gray", vmin=0, vmax=255)
        a.set_title(f"Disparity {title}")
        a.axis("off")
    plt.tight_layout()
    plt.show()
    fig.savefig("disp2.png")

    renderer.destroy_window()

def main():
    SampleCameraPath()

if __name__ == "__main__":
    main()
